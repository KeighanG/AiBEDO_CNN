{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c085fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8996b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Make sure we're in the right directory\n",
    "if os.path.basename(os.getcwd()) in [\"notebooks\", \"examples\"]:\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666b8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/jupyter-dipti/work/processed\"  # the data used for prediction must be here, as well as the cmip6 mean/std statistics\n",
    "#DATA_DIR = \"/home/jupyter-dipti/work/AiBEDO_simultaneousPreds_Salva/Data/Predictions/EOFInput\"\n",
    "# Input data filename (isosph is an order 6 icosahedron, isosph5 of order 5, etc.)\n",
    "filename_input = \"isosph5.nonorm.ERA5_Exp8_Input.nc\"\n",
    "# Output data filename is inferred from the input filename, do not edit!\n",
    "# E.g.: \"compress.isosph.CESM2.historical.r1i1p1f1.Output.nc\"\n",
    "filename_output = filename_input.replace(\"Input.Exp8.nc\", \"Output.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459795ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aibedo.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mccrs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#import proplot as pplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maibedo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maibedo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload_checkpoint_from_wandb, get_run_ids_for_hyperparams\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aibedo.models'"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from typing import *\n",
    "import wandb\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "#import proplot as pplt\n",
    "from aibedo.models import BaseModel\n",
    "from aibedo.utilities.wandb_api import reload_checkpoint_from_wandb, get_run_ids_for_hyperparams\n",
    "import scipy.stats\n",
    "\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from aibedo.utilities.config_utils import get_config_from_hydra_compose_overrides\n",
    "from aibedo.utilities.utils import rsetattr, get_logger, get_local_ckpt_path, rhasattr, rgetattr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c242f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the appropriate device (GPU or CPU) to use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "overrides = [f'datamodule.data_dir={DATA_DIR}', f\"++model.use_auxiliary_vars=False\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe85b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_variables_into_channel_dim(data: xr.Dataset, variables: List[str]) -> np.ndarray:\n",
    "    \"\"\"Concatenate xarray variables into numpy channel dimension (last).\"\"\"\n",
    "    assert len(data[variables[0]].shape) == 2, \"Each input data variable must have two dimensions\"\n",
    "    data_ml = np.concatenate(\n",
    "        [np.expand_dims(data[var].values, axis=-1) for var in variables],\n",
    "        axis=-1  # last axis\n",
    "    )\n",
    "    return data_ml.astype(np.float32)\n",
    "\n",
    "def get_month_of_output_data(output_xarray: xr.Dataset) -> np.ndarray:\n",
    "    \"\"\" Get month of the snapshot (0-11)  \"\"\"\n",
    "    n_gridcells = len(output_xarray['ncells'])\n",
    "    # .item() is required here as only one timestep is used, the subtraction with -1 because we want 0-indexed months\n",
    "    month_of_snapshot = np.array(output_xarray['time.month'], dtype=np.float32) - 1\n",
    "    # now repeat the month for each grid cell/pixel\n",
    "    dataset_month = np.repeat(month_of_snapshot, n_gridcells)\n",
    "    return dataset_month.reshape([month_of_snapshot.shape[0], n_gridcells, 1])  # Add a dummy channel/feature dimension\n",
    "\n",
    "def get_pytorch_model_data(input_xarray: xr.Dataset, output_xarray: xr.Dataset, input_vars: List[str]) -> torch.Tensor:\n",
    "    \"\"\"Get the tensor input data for the ML model.\"\"\"\n",
    "    # Concatenate all variables into the channel/feature dimension (last) of the input tensor\n",
    "    data_input = concat_variables_into_channel_dim(input_xarray, input_vars)\n",
    "    # Get the month of the snapshot (0-11), which is needed to denormalize the model predictions into their original scale\n",
    "    data_month = get_month_of_output_data(output_xarray)\n",
    "    # For convenience, we concatenate the month information to the input data, but it is *not* used by the model!\n",
    "    data_input = np.concatenate([data_input, data_month], axis=-1)\n",
    "    # Convert to torch tensor and move to CPU/GPU\n",
    "    data_input = torch.from_numpy(data_input).float().to(device)\n",
    "    return data_input\n",
    "\n",
    "def predict_with_aibedo_model(aibedo_model: BaseModel, input_tensor: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Predict with the AiBEDO model.\n",
    "    Returns:\n",
    "        A dictionary of output-variable -> prediction-tensor key->value pairs for each variable {var}.\n",
    "        Keys with name {var} (e.g. 'pr') are in denormalized scale. Keys with name {var}_pre or {var}_nonorm are raw predictions of the ML model.\n",
    "        To only get the raw predictions, please use aibedo_model.raw_predict(input_tensor)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # No need to track the gradients during inference\n",
    "        prediction = aibedo_model.predict(input_tensor, return_normalized_outputs=True)  # if true, also return {var}_nonorm (or {var}_pre)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "from aibedo.interface import reload_model_from_config_and_ckpt\n",
    "def load_model(config_path,config_name,ckpt_path,ckpt_name):\n",
    "    overrides = [f'datamodule.data_dir={DATA_DIR}', f\"++model.use_auxiliary_vars=False\"]\n",
    "\n",
    "    ### Load Hydra config file\n",
    "    GlobalHydra.instance().clear() \n",
    "    hydra.initialize(config_path=config_path, version_base=None)\n",
    "    config = hydra.compose(config_name=config_name, overrides=overrides)\n",
    "    config['ckpt_dir'] = ckpt_path\n",
    "    config['callbacks']['model_checkpoint']['dirpath'] = config['ckpt_dir']\n",
    "\n",
    "    ## Modify config dict\n",
    "    if config.model.get('input_transform'):\n",
    "        OmegaConf.update(config, f'model.input_transform._target_',\n",
    "                         str(rgetattr(config, f'model.input_transform._target_')).replace('aibedo_salva', 'aibedo'))\n",
    "    for k in ['model', 'datamodule', 'model.mixer', 'model.input_transform']:\n",
    "        if config.get(k):\n",
    "            OmegaConf.update(config, f'{k}._target_',\n",
    "                             str(rgetattr(config, f'{k}._target_')).replace('aibedo_salva', 'aibedo'))\n",
    "    \n",
    "    ## Load model\n",
    "    loadmodel = reload_model_from_config_and_ckpt(config, ckpt_path+ckpt_name, load_datamodule=True)\n",
    "\n",
    "    return loadmodel[0], config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath='/home/jupyter-dipti/work/AiBEDO_simultaneousPreds_Salva/Data/ckpoints/'\n",
    "#modelsName=os.listdir(dataPath)\n",
    "### Load the actual data and process it\n",
    "#ds_input = xr.open_dataset(f\"{DATA_DIR}/{filename_input}\")  # Input data\n",
    "ds_output = xr.open_dataset(f\"{DATA_DIR}/{filename_output}\") # Ground truth data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdff1f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CESM2-FV2\n",
      "../../Data/ckpoints/CESM2-FV2/wandb/run-20220926_202705-84ad4gyn/files\n",
      "nonorm_0h_epoch013_seed7.ckpt\n",
      "CESM2-WACCM-FV2\n",
      "../../Data/ckpoints/CESM2-WACCM-FV2/wandb/run-20220926_212814-d6et07fq/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "CESM2-WACCM\n",
      "../../Data/ckpoints/CESM2-WACCM/wandb/run-20220926_205901-koxjbllh/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "CESM2\n",
      "../../Data/ckpoints/CESM2/wandb/run-20220926_195019-b8he6k5n/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "CMCC-CM2-SR5\n",
      "../../Data/ckpoints/CMCC-CM2-SR5/wandb/run-20220926_215722-3b6a0h56/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "CanESM5\n",
      "../../Data/ckpoints/CanESM5/wandb/run-20220926_222554-dtgu4t2c/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "FGOALS-g3\n",
      "../../Data/ckpoints/FGOALS-g3/wandb/run-20220926_232552-3no05o85/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "GISS-E2-1-H\n",
      "../../Data/ckpoints/GISS-E2-1-H/wandb/run-20220927_014857-27bwymay/files\n",
      "nonorm_0h_epoch013_seed7.ckpt\n",
      "MIROC-ES2L\n",
      "../../Data/ckpoints/MIROC-ES2L/wandb/run-20220927_121224-3nodghgw/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "MIROC6\n",
      "../../Data/ckpoints/MIROC6/wandb/run-20220927_124043-cicbmrwi/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n",
      "MRI-ESM2-0\n",
      "../../Data/ckpoints/MRI-ESM2-0/wandb/run-20220927_132658-3tplw5gq/files\n",
      "nonorm_0h_epoch013_seed7.ckpt\n",
      "SAM0-UNICON\n",
      "../../Data/ckpoints/SAM0-UNICON/wandb/run-20220927_135545-350kgzxv/files\n",
      "nonorm_0h_epoch014_seed7.ckpt\n"
     ]
    }
   ],
   "source": [
    "dataPath='/home/jupyter-dipti/work/AiBEDO_simultaneousPreds_Salva/Data/ckpoints/'\n",
    "#modelsName=os.listdir(dataPath)\n",
    "### Load the actual data and process it\n",
    "#ds_input = xr.open_dataset(f\"{DATA_DIR}/{filename_input}\")  # Input data\n",
    "ds_output = xr.open_dataset(f\"{DATA_DIR}/{filename_output}\") # Ground truth data\n",
    "\n",
    "gt_era5=xr.open_dataset('/home/jupyter-dipti/work/processed/isosph5.nonorm.ERA5_Exp8_Output.nc')\n",
    "gt_era5\n",
    "\n",
    "gt_tas_nonorm=gt_era5.tas_nonorm\n",
    "gt_ps_nonorm=gt_era5.ps_nonorm\n",
    "gt_pr_nonorm=gt_era5.pr_nonorm\n",
    "\n",
    "\n",
    "#modelsName = [  'FGOALS-g3',  'GISS-E2-1-H',  'MIROC6',   'SAM0-UNICON',  'ERA5',\n",
    "#              'GFDL-ESM4',  'MIROC-ES2L',   'MRI-ESM2-0']#\n",
    "modelsName = [x for x in os.listdir(dataPath) if not x.startswith('.')]\n",
    "for m in modelsName:\n",
    "    if not m=='E3SM-1-1' :\n",
    "        chkPath=dataPath+m+'/checkpoints/'\n",
    "        print(m)\n",
    "        tag=os.listdir(chkPath)[0]\n",
    "        #print(tag)\n",
    "        ckpt_path = chkPath+tag+'/'\n",
    "        #print(ckpt_path)\n",
    "        cnf=dataPath+m+'/wandb/'\n",
    "        extCnf=[x for x in os.listdir(cnf) if (x.startswith('run-2022') & x.endswith(tag))][0]\n",
    "        #print(extCnf)\n",
    "        cnfString='../../Data/ckpoints/'+m+'/wandb/'+extCnf+'/files'\n",
    "        print(cnfString)\n",
    "        #config_path=\n",
    "        #config_path = run-20220927_'++'-cicbmrwi/files'\n",
    "        #print(cnfString)\n",
    "        config_name = 'hydra_config.yaml'\n",
    "        ckptFile=[x for x in os.listdir(ckpt_path) if  x.startswith('nonorm_0h_epoch0')][0]\n",
    "        ckpt_name =ckptFile\n",
    "        print(ckpt_name)\n",
    "        model,config = load_model(cnfString,config_name,ckpt_path,ckpt_name)\n",
    "        inFiles=['netSurfcs_nonorm_othersRegressedOnPC','crelSurf_nonorm_othersRegressedOnPC','cresSurf_nonorm_othersRegressedOnPC',\n",
    "            'netTOAcs_nonorm_othersRegressedOnPC','cres_nonorm_othersRegressedOnPC','crel_nonorm_othersRegressedOnPC']\n",
    "        for inputs in inFiles : #### loop for input files\n",
    "            filename_input='ERA5_Input_EOFs_{0}.nc'.format(inputs) \n",
    "            DATA_DIR2 = \"/home/jupyter-dipti/work/AiBEDO_simultaneousPreds_Salva/Data/Predictions\" \n",
    "            ds_input = xr.open_dataset(f\"{DATA_DIR2}/{filename_input}\")  # Input data            \n",
    "            input_ml = get_pytorch_model_data(ds_input, ds_output, input_vars=model.main_input_vars)\n",
    "    #print(config)\n",
    "    ### Get AiBEDO predictions\n",
    "            predictions_ml = predict_with_aibedo_model(model, input_ml)\n",
    "\n",
    "            tas_nonorm=predictions_ml['tas_nonorm'].numpy()\n",
    "            pred_tas_nonorm=gt_tas_nonorm.copy() ### just to copy coordinates\n",
    "            pred_tas_nonorm.values=tas_nonorm\n",
    "    \n",
    "            ps_nonorm=predictions_ml['ps_nonorm'].numpy()\n",
    "            pred_ps_nonorm=gt_ps_nonorm.copy() ### just to copy coordinates\n",
    "            pred_ps_nonorm.values=ps_nonorm\n",
    "    \n",
    "            pr_nonorm=predictions_ml['pr_nonorm'].numpy()\n",
    "            pred_pr_nonorm=gt_pr_nonorm.copy() ### just to copy coordinates\n",
    "            pred_pr_nonorm.values=pr_nonorm\n",
    "    \n",
    "            output_ds = pred_tas_nonorm.to_dataset(name = 'pred_tas_nonorm')\n",
    "            # Add next DataArray to existing dataset (ds)\n",
    "            output_ds['pred_ps_nonorm'] = pred_ps_nonorm\n",
    "            output_ds['pred_pr_nonorm'] = pred_pr_nonorm\n",
    "            out_path = '/home/jupyter-keighan/work/AiBEDOwork/Simultaneous_Preds/ENSO_FNO_trained_on_{0}_{1}.nc'.format(m,inputs)\n",
    "            output_ds.to_netcdf(path=out_path,mode='w',format='NETCDF4')\n",
    "print('allDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b66ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salva_pred",
   "language": "python",
   "name": "salva_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
